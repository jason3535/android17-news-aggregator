"""
Android 17 News Scraper
çˆ¬å– Android Authority, 9to5Google ä»¥åŠè‘—åçˆ†æ–™äººå£«çš„ Android 17 ç›¸å…³æ–°é—»
"""

import requests
from bs4 import BeautifulSoup
import feedparser
import json
import os
from datetime import datetime
from dateutil import parser as date_parser
import re
import hashlib
from translator import translate_news_batch

DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')
NEWS_FILE = os.path.join(DATA_DIR, 'news.json')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

# Android 17 ç›¸å…³å…³é”®è¯
ANDROID_KEYWORDS = [
    'android 17', 'android17',
    'android baklava',
    'android 16 qpr', 'android16 qpr', 'qpr', 'quarterly platform release',
    'google i/o 2025', 'google i/o 2026',
    'android beta', 'android preview', 'android developer preview',
    'material you', 'gemini android',
    'tensor g5', 'tensor g6',
]

# iOS 27 ç›¸å…³å…³é”®è¯
IOS_KEYWORDS = [
    'ios 27', 'ios27', 'ios 26', 'ios26',
    'ios beta', 'ios preview', 'ios developer',
]

# åˆå¹¶å…³é”®è¯ï¼ˆç”¨äºé€šç”¨è¿‡æ»¤ï¼‰
KEYWORDS = ANDROID_KEYWORDS + IOS_KEYWORDS

def extract_date_from_url(url: str) -> str:
    """ä»URLä¸­æå–æ—¥æœŸï¼ˆå¦‚æœURLåŒ…å«æ—¥æœŸæ¨¡å¼å¦‚/2026/01/28/ï¼‰"""
    # åŒ¹é… /2026/01/28/ æˆ– /2025/12/30/ ç­‰æ ¼å¼
    date_patterns = [
        r'/(\d{4})/(\d{1,2})/(\d{1,2})/',  # /2026/01/28/
        r'/(\d{4})-(\d{1,2})-(\d{1,2})/',  # /2026-01-28/
        r'(\d{4})/(\d{1,2})/(\d{1,2})/',   # 2026/01/28
        r'(\d{4})-(\d{1,2})-(\d{1,2})',    # 2026-01-28
    ]

    for pattern in date_patterns:
        match = re.search(pattern, url)
        if match:
            try:
                year, month, day = match.groups()[:3]
                # ç¡®ä¿æ˜¯æœ‰æ•ˆæ—¥æœŸ
                year = int(year)
                month = int(month)
                day = int(day)
                if 2000 <= year <= 2100 and 1 <= month <= 12 and 1 <= day <= 31:
                    return f"{year:04d}-{month:02d}-{day:02d}"
            except (ValueError, IndexError):
                continue
    return ""

def parse_news_date(date_str: str, url: str = "") -> str:
    """è§£ææ–°é—»æ—¥æœŸï¼Œè¿”å›æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²

    å‚æ•°:
        date_str: åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆæ¥è‡ªRSS feedç­‰ï¼‰
        url: æ–‡ç« URLï¼Œç”¨äºå¤‡ç”¨æ—¥æœŸæå–

    è¿”å›:
        æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼šå¦‚æœæœ‰æ—¶é—´éƒ¨åˆ†åˆ™è¿”å›'YYYY-MM-DD HH:MM'ï¼Œå¦åˆ™è¿”å›'YYYY-MM-DD'
    """
    if not date_str:
        # å°è¯•ä»URLæå–æ—¥æœŸ
        if url:
            url_date = extract_date_from_url(url)
            if url_date:
                return url_date
        return ""

    # å°è¯•å¤šç§è§£ææ–¹å¼
    parsed_date = None

    # æ–¹æ³•1: ä½¿ç”¨dateutil.parser
    try:
        parsed_date = date_parser.parse(date_str)
    except:
        pass

    # æ–¹æ³•2: å°è¯•å¸¸è§æ—¥æœŸæ ¼å¼
    if not parsed_date:
        date_formats = [
            '%Y-%m-%dT%H:%M:%S%z',  # ISOæ ¼å¼å¸¦æ—¶åŒº
            '%Y-%m-%d %H:%M:%S',    # ç®€å•æ—¥æœŸæ—¶é—´
            '%Y-%m-%d',             # ä»…æ—¥æœŸ
            '%a, %d %b %Y %H:%M:%S %z',  # RSSå¸¸è§æ ¼å¼
            '%a, %d %b %Y %H:%M:%S %Z',  # å¸¦æ—¶åŒºåç§°
        ]

        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                break
            except:
                continue

    if parsed_date:
        # æ£€æŸ¥æ˜¯å¦æœ‰éé›¶çš„æ—¶é—´éƒ¨åˆ†
        if parsed_date.hour == 0 and parsed_date.minute == 0 and parsed_date.second == 0:
            # åªæœ‰æ—¥æœŸï¼Œæ²¡æœ‰æ—¶é—´
            return parsed_date.strftime('%Y-%m-%d')
        else:
            # æœ‰æ—¶é—´éƒ¨åˆ†
            return parsed_date.strftime('%Y-%m-%d %H:%M')

    # æ‰€æœ‰è§£æéƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²ï¼ˆæˆªæ–­åˆ°åˆç†é•¿åº¦ï¼‰
    return date_str[:50]

# è‘—åçˆ†æ–™äººå£« Twitter/X è´¦å·
LEAKERS = [
    {'name': 'OnLeaks', 'handle': '@OnLeaks', 'avatar': 'https://pbs.twimg.com/profile_images/1590049827662032896/3Jdz7fGM_400x400.jpg'},
    {'name': 'Evan Blass', 'handle': '@evleaks', 'avatar': 'https://pbs.twimg.com/profile_images/1683602571156635648/NmFNPE3__400x400.jpg'},
    {'name': 'Ice Universe', 'handle': '@UniverseIce', 'avatar': 'https://pbs.twimg.com/profile_images/1590753781534375937/G63Fcoiq_400x400.jpg'},
    {'name': 'Mishaal Rahman', 'handle': '@MishaalRahman', 'avatar': 'https://pbs.twimg.com/profile_images/1772892077495795712/nnAPEaB2_400x400.jpg'},
    {'name': 'Max Weinbach', 'handle': '@MaxWineworthy', 'avatar': 'https://pbs.twimg.com/profile_images/1402848727407013888/6VrpdaKh_400x400.jpg'},
]


def generate_id(url: str) -> str:
    """ç”Ÿæˆæ–°é—»å”¯ä¸€ID"""
    return hashlib.md5(url.encode()).hexdigest()[:12]


def contains_keywords(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯"""
    text_lower = text.lower()
    return any(kw in text_lower for kw in KEYWORDS)


def detect_platform(text: str) -> str:
    """æ£€æµ‹æ–‡æœ¬å±äºå“ªä¸ªå¹³å°"""
    text_lower = text.lower()
    is_android = any(kw in text_lower for kw in ANDROID_KEYWORDS)
    is_ios = any(kw in text_lower for kw in IOS_KEYWORDS)

    if is_android and is_ios:
        return 'android'  # é»˜è®¤ Android
    elif is_ios:
        return 'ios'
    else:
        return 'android'


def extract_image_from_entry(entry) -> str:
    """ä» RSS entry ä¸­æå–å›¾ç‰‡ URL"""
    image_url = None

    # 1. å°è¯•ä» media_content è·å–
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if media.get('medium') == 'image' or media.get('type', '').startswith('image'):
                image_url = media.get('url')
                if image_url:
                    break

    # 2. å°è¯•ä» media_thumbnail è·å–
    if not image_url and hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        image_url = entry.media_thumbnail[0].get('url')

    # 3. å°è¯•ä» enclosure è·å–
    if not image_url and hasattr(entry, 'enclosures') and entry.enclosures:
        for enc in entry.enclosures:
            if enc.get('type', '').startswith('image'):
                image_url = enc.get('href') or enc.get('url')
                if image_url:
                    break

    # 4. å°è¯•ä» content æˆ– summary ä¸­çš„ <img> æ ‡ç­¾è·å–
    if not image_url:
        content = ''
        if hasattr(entry, 'content') and entry.content:
            content = entry.content[0].get('value', '')
        elif hasattr(entry, 'summary'):
            content = entry.summary

        if content:
            soup = BeautifulSoup(content, 'html.parser')
            img_tag = soup.find('img')
            if img_tag:
                image_url = img_tag.get('src') or img_tag.get('data-src')

    return image_url


def fetch_og_image(url: str) -> str:
    """ä»æ–‡ç« é¡µé¢è·å– og:image"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # å°è¯• og:image
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                return og_image['content']
            # å°è¯• twitter:image
            tw_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if tw_image and tw_image.get('content'):
                return tw_image['content']
    except Exception as e:
        print(f"è·å– og:image å¤±è´¥ ({url[:50]}...): {e}")
    return None


def fetch_android_authority() -> list:
    """çˆ¬å– Android Authority çš„ Android æ–°é—»"""
    news = []
    try:
        # ä½¿ç”¨ RSS feed
        feed_url = 'https://www.androidauthority.com/feed/'
        feed = feedparser.parse(feed_url)

        no_match_titles = []
        entries_to_check = feed.entries[:50]  # æ£€æŸ¥æ›´å¤šæ¡ç›®
        for entry in entries_to_check:
            title = entry.get('title', '')
            summary = entry.get('summary', '')
            categories = entry.get('tags', [])
            category_text = ' '.join([tag.get('term', '') for tag in categories if isinstance(tag, dict) and 'term' in tag]) if categories else ''

            # æ£€æŸ¥æ ‡é¢˜ã€æ‘˜è¦å’Œç±»åˆ«æ˜¯å¦åŒ…å«å…³é”®è¯
            if contains_keywords(title) or contains_keywords(summary) or contains_keywords(category_text):
                pub_date = entry.get('published', '')
                date_str = parse_news_date(pub_date, entry.link)

                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': 'Android Authority',
                    'source_icon': 'AA',
                    'date': date_str,
                    'type': 'news',
                    'platform': 'android',
                    'image': image_url
                })
            else:
                no_match_titles.append(title[:60])

        print(f"Android Authority RSS å¤„ç† {len(entries_to_check)} æ¡ï¼ŒåŒ¹é… {len(news)} æ¡")
        if no_match_titles and len(news) == 0:
            print(f"  æœªåŒ¹é…æ ‡é¢˜ç¤ºä¾‹: {no_match_titles[:3]}")
    except Exception as e:
        print(f"Error fetching Android Authority: {e}")

    return news


def fetch_9to5google() -> list:
    """çˆ¬å– 9to5Google çš„ Android æ–°é—»"""
    news = []
    try:
        feed_url = 'https://9to5google.com/feed/'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            if contains_keywords(title) or contains_keywords(summary):
                pub_date = entry.get('published', '')
                date_str = parse_news_date(pub_date, entry.link)

                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': '9to5Google',
                    'source_icon': '9to5',
                    'date': date_str,
                    'type': 'news',
                    'platform': detect_platform(title + ' ' + summary),
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching 9to5Google: {e}")

    return news


def fetch_xda_developers() -> list:
    """çˆ¬å– XDA Developers çš„ Android æ–°é—»"""
    news = []
    try:
        feed_url = 'https://www.xda-developers.com/feed/'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            if contains_keywords(title) or contains_keywords(summary):
                pub_date = entry.get('published', '')
                date_str = parse_news_date(pub_date, entry.link)

                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': 'XDA Developers',
                    'source_icon': 'XDA',
                    'date': date_str,
                    'type': 'news',
                    'platform': 'android',
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching XDA: {e}")

    return news


def fetch_9to5mac() -> list:
    """çˆ¬å– 9to5Mac çš„ iOS æ–°é—»"""
    news = []
    try:
        feed_url = 'https://9to5mac.com/feed/'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            # æ£€æŸ¥æ˜¯å¦åŒ…å« iOS å…³é”®è¯
            text = (title + ' ' + summary).lower()
            if any(kw in text for kw in IOS_KEYWORDS):
                pub_date = entry.get('published', '')
                date_str = parse_news_date(pub_date, entry.link)

                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': '9to5Mac',
                    'source_icon': '9to5',
                    'date': date_str,
                    'type': 'news',
                    'platform': 'ios',
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching 9to5Mac: {e}")

    return news


def fetch_macrumors() -> list:
    """çˆ¬å– MacRumors çš„ iOS æ–°é—»"""
    news = []
    try:
        feed_url = 'https://feeds.macrumors.com/MacRumors-All'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            # æ£€æŸ¥æ˜¯å¦åŒ…å« iOS å…³é”®è¯
            text = (title + ' ' + summary).lower()
            if any(kw in text for kw in IOS_KEYWORDS):
                pub_date = entry.get('published', '')
                date_str = parse_news_date(pub_date, entry.link)

                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': 'MacRumors',
                    'source_icon': 'MR',
                    'date': date_str,
                    'type': 'news',
                    'platform': 'ios',
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching MacRumors: {e}")

    return news


def search_9to5mac(query: str = "ios 27") -> list:
    """é€šè¿‡æœç´¢é¡µé¢çˆ¬å– 9to5Mac çš„æ–‡ç« """
    news = []
    try:
        search_url = f"https://9to5mac.com/?s={query.replace(' ', '+')}"
        response = requests.get(search_url, headers=HEADERS, timeout=15)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = soup.find_all('article', limit=20)

            for article in articles:
                try:
                    title_elem = article.find(['h2', 'h3'])
                    if not title_elem:
                        continue
                    link_elem = title_elem.find('a') or article.find('a')
                    if not link_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')

                    if not url or not title:
                        continue

                    img = article.find('img')
                    image_url = None
                    if img:
                        image_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')

                    time_elem = article.find('time')
                    date_str = ''
                    if time_elem and time_elem.get('datetime'):
                        try:
                            parsed_date = date_parser.parse(time_elem['datetime'])
                            date_str = parsed_date.strftime('%Y-%m-%d %H:%M')
                        except:
                            pass

                    # å¦‚æœä»timeå…ƒç´ æœªæå–åˆ°æ—¥æœŸï¼Œå°è¯•ä»URLæå–
                    if not date_str and url:
                        url_date = extract_date_from_url(url)
                        if url_date:
                            date_str = url_date

                    summary_elem = article.find('p')
                    summary = summary_elem.get_text(strip=True)[:300] if summary_elem else ''

                    news.append({
                        'id': generate_id(url),
                        'title': title,
                        'summary': summary,
                        'url': url,
                        'source': '9to5Mac',
                        'source_icon': '9to5',
                        'date': date_str,
                        'type': 'news',
                        'platform': 'ios',
                        'image': image_url
                    })
                except Exception as e:
                    continue

        print(f"9to5Mac æœç´¢åˆ° {len(news)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"æœç´¢ 9to5Mac å¤±è´¥: {e}")

    return news


def search_android_authority(query: str = "android 17") -> list:
    """é€šè¿‡æœç´¢é¡µé¢çˆ¬å– Android Authority çš„æ–‡ç« """
    news = []
    try:
        search_url = f"https://www.androidauthority.com/?s={query.replace(' ', '+')}"
        response = requests.get(search_url, headers=HEADERS, timeout=15)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾æ–‡ç« 
            articles = []
            selectors = ['article', 'div.post', 'div.entry', 'div.article', 'div.post-item', 'div.news-item']

            for selector in selectors:
                found = soup.select(selector)
                if found:
                    articles = found[:20]  # é™åˆ¶æ•°é‡
                    print(f"Android Authority æœç´¢ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(found)} ä¸ªå…ƒç´ ")
                    break

            if not articles:
                print("Android Authority æœç´¢æœªæ‰¾åˆ°æ–‡ç« å…ƒç´ ")
                return news

            for article in articles:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = article.find(['h2', 'h3'])
                    if not title_elem:
                        continue
                    link_elem = title_elem.find('a') or article.find('a')
                    if not link_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')

                    if not url or not title:
                        continue

                    # æå–å›¾ç‰‡
                    img = article.find('img')
                    image_url = None
                    if img:
                        image_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')

                    # æå–æ—¥æœŸ
                    time_elem = article.find('time')
                    date_str = ''
                    if time_elem and time_elem.get('datetime'):
                        try:
                            parsed_date = date_parser.parse(time_elem['datetime'])
                            date_str = parsed_date.strftime('%Y-%m-%d %H:%M')
                        except:
                            pass

                    # å¦‚æœä»timeå…ƒç´ æœªæå–åˆ°æ—¥æœŸï¼Œå°è¯•ä»URLæå–
                    if not date_str and url:
                        url_date = extract_date_from_url(url)
                        if url_date:
                            date_str = url_date

                    # æå–æ‘˜è¦
                    summary_elem = article.find('p')
                    summary = summary_elem.get_text(strip=True)[:300] if summary_elem else ''

                    news.append({
                        'id': generate_id(url),
                        'title': title,
                        'summary': summary,
                        'url': url,
                        'source': 'Android Authority',
                        'source_icon': 'AA',
                        'date': date_str,
                        'type': 'news',
                        'platform': 'android',
                        'image': image_url
                    })
                except Exception as e:
                    continue

        print(f"Android Authority æœç´¢åˆ° {len(news)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"æœç´¢ Android Authority å¤±è´¥: {e}")

    return news


def search_9to5google(query: str = "android 17") -> list:
    """é€šè¿‡æœç´¢é¡µé¢çˆ¬å– 9to5Google çš„æ–‡ç« """
    news = []
    try:
        search_url = f"https://9to5google.com/?s={query.replace(' ', '+')}"
        response = requests.get(search_url, headers=HEADERS, timeout=15)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = soup.find_all('article', limit=20)

            for article in articles:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = article.find(['h2', 'h3'])
                    if not title_elem:
                        continue
                    link_elem = title_elem.find('a') or article.find('a')
                    if not link_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')

                    if not url or not title:
                        continue

                    # æå–å›¾ç‰‡
                    img = article.find('img')
                    image_url = None
                    if img:
                        image_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')

                    # æå–æ—¥æœŸ
                    time_elem = article.find('time')
                    date_str = ''
                    if time_elem and time_elem.get('datetime'):
                        try:
                            parsed_date = date_parser.parse(time_elem['datetime'])
                            date_str = parsed_date.strftime('%Y-%m-%d %H:%M')
                        except:
                            pass

                    # å¦‚æœä»timeå…ƒç´ æœªæå–åˆ°æ—¥æœŸï¼Œå°è¯•ä»URLæå–
                    if not date_str and url:
                        url_date = extract_date_from_url(url)
                        if url_date:
                            date_str = url_date

                    # æå–æ‘˜è¦
                    summary_elem = article.find('p')
                    summary = summary_elem.get_text(strip=True)[:300] if summary_elem else ''

                    news.append({
                        'id': generate_id(url),
                        'title': title,
                        'summary': summary,
                        'url': url,
                        'source': '9to5Google',
                        'source_icon': '9to5',
                        'date': date_str,
                        'type': 'news',
                        'platform': detect_platform(title + ' ' + summary),
                        'image': image_url
                    })
                except Exception as e:
                    continue

        print(f"9to5Google æœç´¢åˆ° {len(news)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"æœç´¢ 9to5Google å¤±è´¥: {e}")

    return news


def get_leaker_info() -> list:
    """è¿”å›è‘—åçˆ†æ–™äººå£«ä¿¡æ¯ï¼ˆä¾›å‰ç«¯æ˜¾ç¤ºï¼‰"""
    return [
        {
            'id': f'leaker_{i}',
            'title': f"{leaker['name']} ({leaker['handle']})",
            'summary': f"å…³æ³¨ {leaker['handle']} è·å–æœ€æ–° Android çˆ†æ–™ä¿¡æ¯ã€‚ç”±äº Twitter/X API é™åˆ¶ï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹å…¶è´¦å·è·å–æœ€æ–°æ¶ˆæ¯ã€‚",
            'url': f"https://twitter.com/{leaker['handle'].replace('@', '')}",
            'source': 'Leaker',
            'source_icon': 'ğŸ”¥',
            'date': datetime.now().strftime('%Y-%m-%d'),
            'type': 'leaker',
            'image': leaker.get('avatar')
        }
        for i, leaker in enumerate(LEAKERS)
    ]


def scrape_all() -> dict:
    """çˆ¬å–æ‰€æœ‰æ¥æºçš„æ–°é—»ï¼Œä¿ç•™å†å²æ•°æ®"""

    # 1. åŠ è½½å†å²æ•°æ®
    existing_data = load_news()
    existing_news = {item['id']: item for item in existing_data.get('items', []) if item['type'] == 'news'}
    print(f"å·²æœ‰å†å²æ–°é—»: {len(existing_news)} æ¡")

    # 2. çˆ¬å–æ–°æ•°æ®ï¼ˆRSS + æœç´¢ï¼‰
    new_news = []

    # RSS feeds
    print("çˆ¬å– RSS feeds...")
    new_news.extend(fetch_android_authority())
    new_news.extend(fetch_9to5google())
    new_news.extend(fetch_xda_developers())

    # iOS RSS feeds
    print("çˆ¬å– iOS RSS feeds...")
    new_news.extend(fetch_9to5mac())
    new_news.extend(fetch_macrumors())

    # æœç´¢é¡µé¢ï¼ˆè·å–æ›´å¤šå†å²æ–‡ç« ï¼‰
    print("æœç´¢ Android å†å²æ–‡ç« ...")
    new_news.extend(search_android_authority("android 17"))
    new_news.extend(search_9to5google("android 17"))
    new_news.extend(search_android_authority("android 17 beta"))
    new_news.extend(search_9to5google("android 17 beta"))
    new_news.extend(search_android_authority("android 17 preview"))
    new_news.extend(search_9to5google("android 17 preview"))
    new_news.extend(search_android_authority("android baklava"))
    new_news.extend(search_9to5google("android baklava"))
    new_news.extend(search_android_authority("android 16 qpr"))
    new_news.extend(search_9to5google("android 16 qpr"))
    new_news.extend(search_android_authority("qpr"))
    new_news.extend(search_9to5google("qpr"))

    # iOS æœç´¢
    print("æœç´¢ iOS å†å²æ–‡ç« ...")
    new_news.extend(search_9to5mac("ios 27"))
    new_news.extend(search_9to5mac("iphone 17"))

    # 3. åˆå¹¶æ–°æ—§æ•°æ®ï¼ˆå»é‡ï¼Œä»¥ id ä¸ºå‡†ï¼‰
    new_count = 0
    for item in new_news:
        if item['id'] not in existing_news:
            existing_news[item['id']] = item
            new_count += 1

    print(f"æ–°å¢æ–°é—»: {new_count} æ¡")

    # 4. è½¬æ¢å›åˆ—è¡¨å¹¶æ’åº
    news_items = list(existing_news.values())
    try:
        news_items.sort(key=lambda x: x['date'], reverse=True)
    except:
        pass

    # 4.5 è¡¥å……ç¼ºå¤±çš„å›¾ç‰‡ï¼ˆä»æ–‡ç« é¡µé¢è·å– og:imageï¼‰
    print("è¡¥å……ç¼ºå¤±å›¾ç‰‡...")
    for item in news_items:
        if not item.get('image') and item.get('url'):
            image = fetch_og_image(item['url'])
            if image:
                item['image'] = image
                print(f"  è·å–å›¾ç‰‡: {item['title'][:40]}...")

    # 4.6 ä¸¥æ ¼è¿‡æ»¤ Android 17 æ–°é—»ï¼ˆæ¸…ç†é Android 17 å†…å®¹ï¼‰
    print("ä¸¥æ ¼è¿‡æ»¤ Android 17 æ–°é—»...")
    # ä¿ç•™çš„å…³é”®è¯ï¼šAndroid 17 å’Œç›¸å…³ç‰ˆæœ¬
    keep_keywords = ['android 17', 'android17', 'android baklava', 'android 16 qpr', 'qpr', 'quarterly platform release']
    # éœ€è¦è¿‡æ»¤çš„å•çº¯ç¡¬ä»¶å…³é”®è¯ï¼ˆå¦‚æœæ²¡æœ‰ç‰ˆæœ¬å…³é”®è¯ï¼‰
    hardware_keywords = ['pixel 10', 'pixel 11', 'pixel 10a', 'pixel10', 'pixel11']

    filtered_items = []
    for item in news_items:
        if item.get('type') == 'news' and item.get('platform') == 'android':
            text = (item.get('title', '') + ' ' + item.get('summary', '')).lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦ä¿ç•™çš„ Android ç‰ˆæœ¬å…³é”®è¯
            has_version = any(kw in text for kw in keep_keywords)
            # æ£€æŸ¥æ˜¯å¦åªåŒ…å«ç¡¬ä»¶å…³é”®è¯
            has_hardware = any(kw in text for kw in hardware_keywords)

            if has_version:
                # åŒ…å« Android ç‰ˆæœ¬å…³é”®è¯ï¼Œä¿ç•™
                filtered_items.append(item)
            elif has_hardware:
                # åªåŒ…å«ç¡¬ä»¶å…³é”®è¯ï¼ˆå¦‚ Pixelï¼‰ï¼Œæ²¡æœ‰ç‰ˆæœ¬å…³é”®è¯ï¼Œè¿‡æ»¤æ‰
                print(f"  ç§»é™¤å•çº¯ç¡¬ä»¶æ–°é—»: {item.get('title', '')[:50]}...")
            else:
                # æ—¢æ²¡æœ‰ç‰ˆæœ¬å…³é”®è¯ä¹Ÿæ²¡æœ‰ç¡¬ä»¶å…³é”®è¯ï¼Œå¯èƒ½æ˜¯å…¶ä»– Android æ–°é—»ï¼Œè¿‡æ»¤æ‰
                print(f"  ç§»é™¤é Android 17 æ–°é—»: {item.get('title', '')[:50]}...")
        else:
            filtered_items.append(item)  # ä¿ç•™ iOS æ–°é—»å’Œçˆ†æ–™äººå£«
    news_items = filtered_items

    # 4.7 ä¸¥æ ¼è¿‡æ»¤ iOS æ–°é—»ï¼ˆæ¸…ç†é iOS 27/26 beta å†…å®¹ï¼‰
    print("ä¸¥æ ¼è¿‡æ»¤ iOS æ–°é—»...")
    # ä¿ç•™çš„å…³é”®è¯ï¼šiOS ç‰ˆæœ¬ç›¸å…³
    ios_keep_keywords = ['ios 27', 'ios27', 'ios 26', 'ios26', 'ios beta', 'ios preview', 'ios developer']
    # éœ€è¦è¿‡æ»¤çš„ç¡¬ä»¶å…³é”®è¯ï¼ˆå¦‚æœæ²¡æœ‰ç‰ˆæœ¬å…³é”®è¯ï¼‰
    ios_hardware_keywords = ['iphone 17', 'iphone17', 'iphone 18']

    ios_filtered_items = []
    for item in news_items:
        if item.get('type') == 'news' and item.get('platform') == 'ios':
            text = (item.get('title', '') + ' ' + item.get('summary', '')).lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦ä¿ç•™çš„ iOS ç‰ˆæœ¬å…³é”®è¯
            has_ios_version = any(kw in text for kw in ios_keep_keywords)
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¡¬ä»¶å…³é”®è¯
            has_ios_hardware = any(kw in text for kw in ios_hardware_keywords)

            if has_ios_version:
                # åŒ…å« iOS ç‰ˆæœ¬å…³é”®è¯ï¼Œä¿ç•™
                ios_filtered_items.append(item)
            elif has_ios_hardware:
                # åªåŒ…å«ç¡¬ä»¶å…³é”®è¯ï¼ˆå¦‚ iPhoneï¼‰ï¼Œæ²¡æœ‰ç‰ˆæœ¬å…³é”®è¯ï¼Œè¿‡æ»¤æ‰
                print(f"  ç§»é™¤å•çº¯ iPhone ç¡¬ä»¶æ–°é—»: {item.get('title', '')[:50]}...")
            else:
                # æ—¢æ²¡æœ‰ç‰ˆæœ¬å…³é”®è¯ä¹Ÿæ²¡æœ‰ç¡¬ä»¶å…³é”®è¯ï¼Œå¯èƒ½æ˜¯å…¶ä»– iOS æ–°é—»ï¼Œè¿‡æ»¤æ‰
                print(f"  ç§»é™¤é iOS 27/26 beta æ–°é—»: {item.get('title', '')[:50]}...")
        else:
            ios_filtered_items.append(item)  # ä¿ç•™ Android æ–°é—»å’Œçˆ†æ–™äººå£«
    news_items = ios_filtered_items

    # 5. æ·»åŠ çˆ†æ–™äººå£«ä¿¡æ¯ï¼ˆæ¯æ¬¡æ›´æ–°ï¼‰
    leaker_items = get_leaker_info()

    # 6. åˆå¹¶ç»“æœ
    sorted_news = news_items + leaker_items

    # 7. ä¸ºæ–°å¢æ–°é—»é¡¹æ·»åŠ ä¸­æ–‡ç¿»è¯‘
    print("å¼€å§‹é¢„ç¿»è¯‘ä¸­æ–‡å†…å®¹...")
    try:
        # åªç¿»è¯‘æ²¡æœ‰ç¿»è¯‘è¿‡çš„æ–°é—»
        news_to_translate = [item for item in news_items if not item.get('title_translated')]
        if news_to_translate:
            print(f"éœ€è¦ç¿»è¯‘: {len(news_to_translate)} æ¡")
            translated_news = translate_news_batch(news_to_translate, 'zh-CN')
            # æ›´æ–°ç¿»è¯‘å­—æ®µ
            for i, item in enumerate(news_to_translate):
                item.update(translated_news[i])
    except Exception as e:
        print(f"é¢„ç¿»è¯‘å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨å®æ—¶ç¿»è¯‘: {e}")

    # ç»Ÿè®¡å„å¹³å°æ•°é‡
    android_count = len([item for item in news_items if item.get('platform') == 'android'])
    ios_count = len([item for item in news_items if item.get('platform') == 'ios'])

    result = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_count': len(sorted_news),
        'news_count': len(news_items),
        'android_count': android_count,
        'ios_count': ios_count,
        'leaker_count': len(leaker_items),
        'items': sorted_news
    }

    # 8. ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    return result


def load_news() -> dict:
    """åŠ è½½å·²ä¿å­˜çš„æ–°é—»æ•°æ®"""
    if os.path.exists(NEWS_FILE):
        with open(NEWS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {'items': [], 'last_updated': None, 'total_count': 0}


if __name__ == '__main__':
    print("å¼€å§‹çˆ¬å– Android 17 æ–°é—»...")
    result = scrape_all()
    print(f"å®Œæˆï¼å…±è·å– {result['total_count']} æ¡å†…å®¹")
    print(f"å…¶ä¸­æ–°é—» {result['news_count']} æ¡ï¼Œçˆ†æ–™äººå£« {result['leaker_count']} ä½")
