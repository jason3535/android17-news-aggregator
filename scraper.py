"""
Android 17 News Scraper
çˆ¬å– Android Authority, 9to5Google ä»¥åŠè‘—åçˆ†æ–™äººå£«çš„ Android 17 ç›¸å…³æ–°é—»
"""

import requests
from bs4 import BeautifulSoup
import feedparser
import json
import os
from datetime import datetime
from dateutil import parser as date_parser
import re
import hashlib

DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')
NEWS_FILE = os.path.join(DATA_DIR, 'news.json')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

# Android 17 ç›¸å…³å…³é”®è¯
KEYWORDS = [
    'android 17', 'android17', 'android 16', 'android 2025', 'android 2026',
    'pixel 10', 'pixel 11', 'android baklava', 'android dessert',
    'google i/o 2025', 'google i/o 2026', 'android beta', 'android preview'
]

# è‘—åçˆ†æ–™äººå£« Twitter/X è´¦å·
LEAKERS = [
    {'name': 'OnLeaks', 'handle': '@OnLeaks', 'avatar': 'https://pbs.twimg.com/profile_images/1590049827662032896/3Jdz7fGM_400x400.jpg'},
    {'name': 'Evan Blass', 'handle': '@evleaks', 'avatar': 'https://pbs.twimg.com/profile_images/1683602571156635648/NmFNPE3__400x400.jpg'},
    {'name': 'Ice Universe', 'handle': '@UniverseIce', 'avatar': 'https://pbs.twimg.com/profile_images/1590753781534375937/G63Fcoiq_400x400.jpg'},
    {'name': 'Mishaal Rahman', 'handle': '@MishaalRahman', 'avatar': 'https://pbs.twimg.com/profile_images/1772892077495795712/nnAPEaB2_400x400.jpg'},
    {'name': 'Max Weinbach', 'handle': '@MaxWineworthy', 'avatar': 'https://pbs.twimg.com/profile_images/1402848727407013888/6VrpdaKh_400x400.jpg'},
]


def generate_id(url: str) -> str:
    """ç”Ÿæˆæ–°é—»å”¯ä¸€ID"""
    return hashlib.md5(url.encode()).hexdigest()[:12]


def contains_keywords(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å« Android 17 ç›¸å…³å…³é”®è¯"""
    text_lower = text.lower()
    return any(kw in text_lower for kw in KEYWORDS)


def extract_image_from_entry(entry) -> str:
    """ä» RSS entry ä¸­æå–å›¾ç‰‡ URL"""
    image_url = None

    # 1. å°è¯•ä» media_content è·å–
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if media.get('medium') == 'image' or media.get('type', '').startswith('image'):
                image_url = media.get('url')
                if image_url:
                    break

    # 2. å°è¯•ä» media_thumbnail è·å–
    if not image_url and hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        image_url = entry.media_thumbnail[0].get('url')

    # 3. å°è¯•ä» enclosure è·å–
    if not image_url and hasattr(entry, 'enclosures') and entry.enclosures:
        for enc in entry.enclosures:
            if enc.get('type', '').startswith('image'):
                image_url = enc.get('href') or enc.get('url')
                if image_url:
                    break

    # 4. å°è¯•ä» content æˆ– summary ä¸­çš„ <img> æ ‡ç­¾è·å–
    if not image_url:
        content = ''
        if hasattr(entry, 'content') and entry.content:
            content = entry.content[0].get('value', '')
        elif hasattr(entry, 'summary'):
            content = entry.summary

        if content:
            soup = BeautifulSoup(content, 'html.parser')
            img_tag = soup.find('img')
            if img_tag:
                image_url = img_tag.get('src') or img_tag.get('data-src')

    # 5. å°è¯•ä» link çš„ og:image è·å–ï¼ˆå¤‡ç”¨ï¼Œè¾ƒæ…¢ï¼‰
    # æš‚ä¸å¯ç”¨ï¼Œå› ä¸ºä¼šå¢åŠ è¯·æ±‚æ—¶é—´

    return image_url


def fetch_android_authority() -> list:
    """çˆ¬å– Android Authority çš„ Android æ–°é—»"""
    news = []
    try:
        # ä½¿ç”¨ RSS feed
        feed_url = 'https://www.androidauthority.com/feed/'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            if contains_keywords(title) or contains_keywords(summary):
                pub_date = entry.get('published', '')
                try:
                    parsed_date = date_parser.parse(pub_date)
                    date_str = parsed_date.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = pub_date

                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': 'Android Authority',
                    'source_icon': 'AA',
                    'date': date_str,
                    'type': 'news',
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching Android Authority: {e}")

    return news


def fetch_9to5google() -> list:
    """çˆ¬å– 9to5Google çš„ Android æ–°é—»"""
    news = []
    try:
        feed_url = 'https://9to5google.com/feed/'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            if contains_keywords(title) or contains_keywords(summary):
                pub_date = entry.get('published', '')
                try:
                    parsed_date = date_parser.parse(pub_date)
                    date_str = parsed_date.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = pub_date

                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': '9to5Google',
                    'source_icon': '9to5',
                    'date': date_str,
                    'type': 'news',
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching 9to5Google: {e}")

    return news


def fetch_xda_developers() -> list:
    """çˆ¬å– XDA Developers çš„ Android æ–°é—»"""
    news = []
    try:
        feed_url = 'https://www.xda-developers.com/feed/'
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:30]:
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            if contains_keywords(title) or contains_keywords(summary):
                pub_date = entry.get('published', '')
                try:
                    parsed_date = date_parser.parse(pub_date)
                    date_str = parsed_date.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = pub_date

                # æå–å›¾ç‰‡
                image_url = extract_image_from_entry(entry)

                news.append({
                    'id': generate_id(entry.link),
                    'title': title,
                    'summary': BeautifulSoup(summary, 'html.parser').get_text()[:300],
                    'url': entry.link,
                    'source': 'XDA Developers',
                    'source_icon': 'XDA',
                    'date': date_str,
                    'type': 'news',
                    'image': image_url
                })
    except Exception as e:
        print(f"Error fetching XDA: {e}")

    return news


def get_leaker_info() -> list:
    """è¿”å›è‘—åçˆ†æ–™äººå£«ä¿¡æ¯ï¼ˆä¾›å‰ç«¯æ˜¾ç¤ºï¼‰"""
    return [
        {
            'id': f'leaker_{i}',
            'title': f"{leaker['name']} ({leaker['handle']})",
            'summary': f"å…³æ³¨ {leaker['handle']} è·å–æœ€æ–° Android çˆ†æ–™ä¿¡æ¯ã€‚ç”±äº Twitter/X API é™åˆ¶ï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹å…¶è´¦å·è·å–æœ€æ–°æ¶ˆæ¯ã€‚",
            'url': f"https://twitter.com/{leaker['handle'].replace('@', '')}",
            'source': 'Leaker',
            'source_icon': 'ğŸ”¥',
            'date': datetime.now().strftime('%Y-%m-%d'),
            'type': 'leaker',
            'image': leaker.get('avatar')
        }
        for i, leaker in enumerate(LEAKERS)
    ]


def scrape_all() -> dict:
    """çˆ¬å–æ‰€æœ‰æ¥æºçš„æ–°é—»"""
    all_news = []

    # çˆ¬å–å„ä¸ªæ¥æº
    all_news.extend(fetch_android_authority())
    all_news.extend(fetch_9to5google())
    all_news.extend(fetch_xda_developers())

    # æ·»åŠ çˆ†æ–™äººå£«ä¿¡æ¯
    all_news.extend(get_leaker_info())

    # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°é—»ç±»å‹ï¼‰
    news_items = [n for n in all_news if n['type'] == 'news']
    leaker_items = [n for n in all_news if n['type'] == 'leaker']

    # å¯¹æ–°é—»æŒ‰æ—¥æœŸæ’åº
    try:
        news_items.sort(key=lambda x: x['date'], reverse=True)
    except:
        pass

    # åˆå¹¶ç»“æœ
    sorted_news = news_items + leaker_items

    result = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_count': len(sorted_news),
        'news_count': len(news_items),
        'leaker_count': len(leaker_items),
        'items': sorted_news
    }

    # ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    return result


def load_news() -> dict:
    """åŠ è½½å·²ä¿å­˜çš„æ–°é—»æ•°æ®"""
    if os.path.exists(NEWS_FILE):
        with open(NEWS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {'items': [], 'last_updated': None, 'total_count': 0}


if __name__ == '__main__':
    print("å¼€å§‹çˆ¬å– Android 17 æ–°é—»...")
    result = scrape_all()
    print(f"å®Œæˆï¼å…±è·å– {result['total_count']} æ¡å†…å®¹")
    print(f"å…¶ä¸­æ–°é—» {result['news_count']} æ¡ï¼Œçˆ†æ–™äººå£« {result['leaker_count']} ä½")
